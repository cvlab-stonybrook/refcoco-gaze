# RefCOCO-Gaze

![refcocogaze-teaser](https://drive.google.com/uc?export=view&id=1U67rmU4MLBOCc9ASAnIttwlZhXdwPoUN)

We introduce a large-scale dataset of speech-driven human eye movements, where they see an image and hear a referring expression defining the object in the scene (e.g., guy in back left wearing black). 

RefCOCO-Gaze is a laboratory-quality dataset large enough to train deep-network models, including 19,738 human gaze scanpaths, corresponding to 2,094 unique image-expression pairs, from 220 participants performing object referral task.

RefCOCO-Gaze aims to advance research in human gaze prediction by moving beyond simple visual tasks (such as free-viewing or search) to more naturalistic and ecologically valid contexts that use language. We hope this dataset facilitates the development of a computational model that predicts and explains how spoken language guides moment-by-moment human attention control. This understanding will benefit various HCI systems that must effectively interact with humans by predicting their gaze in natural multimodal setups.

We hope you enjoy using RefCOCO-Gaze! ðŸŽ‰
